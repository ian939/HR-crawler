import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
import re
import os

# --- í™˜ê²½ ì„¤ì • ---
SLACK_WEBHOOK_URL = os.environ.get('SLACK_WEBHOOK_URL')
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
}

def fetch_detail_content(url):
    """ìƒì„¸ í˜ì´ì§€ ë³¸ë¬¸ ì¶”ì¶œ (ì‚¬ëŒì¸ ìš°íšŒ ë° ë…¸ì´ì¦ˆ ì œê±° ê°•í™”)"""
    try:
        time.sleep(2)
        target_url = url
        
        # [ìˆ˜ì •] ì‚¬ëŒì¸ì˜ ê²½ìš° iframe ë³¸ë¬¸ ì£¼ì†Œë¡œ ê°•ì œ ì „í™˜í•˜ì—¬ 'ë¡œê·¸ì¸' ë©”ì‹œì§€ íšŒí”¼
        if "saramin.co.kr" in url and "rec_idx=" in url:
            rec_idx_match = re.search(r'rec_idx=(\d+)', url)
            if rec_idx_match:
                target_url = f"https://www.saramin.co.kr/zf_user/jobs/relay/view-detail?rec_idx={rec_idx_match.group(1)}"

        res = requests.get(target_url, headers=HEADERS, timeout=15)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')

        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for tag in soup(["script", "style", "nav", "footer", "header", "button", "aside", "iframe"]):
            tag.decompose()

        # ë³¸ë¬¸ í•µì‹¬ ì„ íƒì ë¦¬ìŠ¤íŠ¸
        selectors = [
            '.user_content', '.recruit_view_cont', '.view_con', 
            '.job_detail', '.template_area', '.cont_jview', '.wrap_jv_cont'
        ]
        
        content_text = ""
        for sel in selectors:
            target = soup.select_one(sel)
            if target:
                content_text = target.get_text(separator="\n", strip=True)
                break
        
        # ì„ íƒìë¡œ ëª» ì°¾ì€ ê²½ìš° ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
        if not content_text or len(content_text) < 100:
            content_text = soup.get_text(separator="\n", strip=True)

        # [ìˆ˜ì •] ë…¸ì´ì¦ˆ í•„í„°ë§ (ë¡œê·¸ì¸ ê´€ë ¨ í…ìŠ¤íŠ¸ê°€ ë³¸ë¬¸ì˜ ì£¼ê°€ ë˜ë©´ ë¬´íš¨ ì²˜ë¦¬)
        noise_keywords = ["ë¡œê·¸ì¸", "íšŒì›ê°€ì…", "ì•„ì´ë”” ì°¾ê¸°", "ë¹„ë°€ë²ˆí˜¸ ì°¾ê¸°", "ëª¨ë°”ì¼ë„¤íŠ¸ì›Œí¬"]
        if any(k in content_text[:200] for k in noise_keywords) and len(content_text) < 500:
            return "ë³¸ë¬¸ ë‚´ìš© í™•ì¸ ë¶ˆê°€ (ìƒì„¸ í˜ì´ì§€ ë§í¬ ì°¸ì¡°)"

        return content_text[:15000] # ì €ì¥ ìš©ëŸ‰ ì œí•œ
    except Exception as e:
        return f"ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"

def get_bep_jobs():
    """BEP ìˆ˜ì§‘ ë¡œì§ - ì „ì²´ ëª©ë¡ ê¸°ë°˜ í‚¤ì›Œë“œ í•„í„°ë§ (ëˆ„ë½ ë°©ì§€)"""
    url = "https://bep.co.kr/Career/recruitment"
    jobs = []
    try:
        response = requests.get(url, headers=HEADERS)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ëª¨ë“  ê³µê³  ë§í¬(recruitmentView?idx=) íƒìƒ‰
        links = soup.find_all('a', href=re.compile(r'recruitmentView\?idx='))
        for l in links:
            text = l.get_text(" ", strip=True)
            # ìƒíƒœê°’(ëª¨ì§‘ì¤‘)ê³¼ í‚¤ì›Œë“œ(ì „ê¸°ì°¨/ì¶©ì „/ìš´ì˜/ì›Œí„°) ë™ì‹œ í™•ì¸
            if "ëª¨ì§‘ì¤‘" in text and any(k in text for k in ["ì „ê¸°ì°¨", "ì¶©ì „", "ì›Œí„°", "WATER", "ìš´ì˜", "ë§¤ë‹ˆì €"]):
                href = l.get('href', '')
                full_link = f"https://bep.co.kr{href}" if not href.startswith('http') else href
                title = text.replace("ëª¨ì§‘ì¤‘", "").strip()
                jobs.append(['BEP', title, "ê³µê³  ì°¸ì¡°", full_link])
    except Exception as e:
        print(f"BEP í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    return jobs

def get_saramin_jobs(companies):
    """ì‚¬ëŒì¸ íŠ¹ì • ê¸°ì—… ìˆ˜ì§‘"""
    base_url = "https://www.saramin.co.kr/zf_user/search/recruit"
    jobs = []
    for company in companies:
        try:
            params = {'searchword': company, 'searchType': 'search'}
            res = requests.get(base_url, headers=HEADERS, params=params)
            soup = BeautifulSoup(res.text, 'html.parser')
            items = soup.select('.item_recruit')
            for item in items:
                co_tag = item.select_one('.corp_name a')
                if not co_tag: continue
                co_name = co_tag.text.strip()
                if company in co_name.replace("(ì£¼)", "").replace("ì£¼ì‹íšŒì‚¬", ""):
                    title_tag = item.select_one('.job_tit a')
                    link = "https://www.saramin.co.kr" + title_tag['href']
                    jobs.append([co_name, title_tag.text.strip(), "ê³µê³  ì°¸ì¡°", link])
            time.sleep(1.5)
        except: continue
    return jobs

def send_slack_message(new_jobs):
    if not SLACK_WEBHOOK_URL or not new_jobs: return
    message = f"ğŸ“¢ *ì‹ ê·œ ì „ê¸°ì°¨ ì¶©ì „ ì±„ìš© ê³µê³  ({len(new_jobs)}ê±´)*\n\n"
    for job in new_jobs:
        message += f"â€¢ *[{job[0]}]* {job[1]}\n  <{job[3]}|ê³µê³  ë³´ê¸°>\n\n"
    requests.post(SLACK_WEBHOOK_URL, json={"text": message})

def main():
    target_companies = ["ëŒ€ì˜ì±„ë¹„", "ì´ë¸Œì´ì‹œìŠ¤", "í”ŒëŸ¬ê·¸ë§í¬", "ë³¼íŠ¸ì—…", "ì°¨ì§€ë¹„", "ì—ë²„ì˜¨"]
    today = datetime.now().strftime('%Y-%m-%d')
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    master_file = "job_listings_all.csv"
    comp_file = "Recruitment_completed.csv"
    ency_file = "encyclopedia.csv"

    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    df_master = pd.read_csv(master_file) if os.path.exists(master_file) else pd.DataFrame(columns=['company', 'title', 'experience', 'link', 'first_seen'])
    df_ency = pd.read_csv(ency_file) if os.path.exists(ency_file) else pd.DataFrame(columns=['link', 'company', 'title', 'content', 'last_updated'])
    df_comp = pd.read_csv(comp_file) if os.path.exists(comp_file) else pd.DataFrame(columns=['company', 'title', 'experience', 'link', 'completed_date'])

    # 1. í¬ë¡¤ë§ ìˆ˜í–‰
    print("ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    bep_data = get_bep_jobs()
    saram_data = get_saramin_jobs(target_companies)
    current_jobs = bep_data + saram_data
    df_current = pd.DataFrame(current_jobs, columns=['company', 'title', 'experience', 'link'])

    # [ì¤‘ìš”] ì•ˆì „ì¥ì¹˜: ìˆ˜ì§‘ì— ì„±ê³µí•œ íšŒì‚¬ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
    successful_scan_companies = df_current['company'].unique()

    # 2. ì‹ ê·œ ê³µê³  ì²˜ë¦¬ (Master ì—…ë°ì´íŠ¸)
    new_entries = df_current[~df_current['link'].isin(df_master['link'])].copy()
    if not new_entries.empty:
        new_entries['first_seen'] = today
        send_slack_message(new_entries.values.tolist())
        df_master = pd.concat([df_master, new_entries], ignore_index=True)

    # 3. ì±„ìš© ì¢…ë£Œ ì²˜ë¦¬ (ì •êµí•œ ë¡œì§ ì ìš©)
    # ë¡œì§: ë§ˆìŠ¤í„°ì—ëŠ” ìˆìœ¼ë‚˜ ì˜¤ëŠ˜ í¬ë¡¤ë§ ê²°ê³¼ì—ëŠ” ì—†ëŠ” ê³µê³ 
    is_missing = ~df_master['link'].isin(df_current['link'])
    # í•˜ì§€ë§Œ í•´ë‹¹ íšŒì‚¬ ìì²´ê°€ ì˜¤ëŠ˜ í¬ë¡¤ë§ì—ì„œ ë‹¨ í•œ ê±´ë„ ë°œê²¬ë˜ì§€ ì•Šì•˜ë‹¤ë©´ 'ìˆ˜ì§‘ ì‹¤íŒ¨'ë¡œ ê°„ì£¼í•˜ê³  ë³´ë¥˜
    is_safe_to_close = df_master['company'].isin(successful_scan_companies)
    
    closed_entries = df_master[is_missing & is_safe_to_close].copy()
    
    if not closed_entries.empty:
        closed_entries['completed_date'] = today
        df_comp = pd.concat([df_comp, closed_entries], ignore_index=True)
        # ë§ˆìŠ¤í„°ì—ì„œ ì‹¤ì œë¡œ ì œê±°
        df_master = df_master[~(is_missing & is_safe_to_close)]
        print(f"{len(closed_entries)}ê±´ì˜ ì±„ìš© ì¢…ë£Œê°€ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 4. ë°±ê³¼ì‚¬ì „(Encyclopedia) ë° ë³¸ë¬¸ ì—…ë°ì´íŠ¸
    # ëŒ€ìƒ: ë°±ê³¼ì‚¬ì „ì— ì•„ì˜ˆ ì—†ê±°ë‚˜, ê¸°ì¡´ ë‚´ìš©ì´ 'ì‹¤íŒ¨' í˜¹ì€ 'ë¡œê·¸ì¸' ê´€ë ¨ì¸ ê²½ìš°
    failed_keywords = ["ë³¸ë¬¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ë¡œê·¸ì¸", "ìˆ˜ì§‘ ì‹¤íŒ¨", "í™•ì¸ ë¶ˆê°€"]
    is_failed_content = df_ency['content'].apply(lambda x: any(k in str(x) for k in failed_keywords))
    
    # 4-1. ê¸°ì¡´ ë°±ê³¼ì‚¬ì „ì—ì„œ ì‹¤íŒ¨í•œ ë§í¬ë“¤ ì¶”ì¶œ
    links_to_retry = df_ency[is_failed_content]['link'].tolist()
    # 4-2. ì˜¤ëŠ˜ í¬ë¡¤ë§ ëœ ê²ƒ ì¤‘ ë°±ê³¼ì‚¬ì „ì— ì•„ì˜ˆ ì—†ëŠ” ë§í¬ë“¤
    links_to_add = df_current[~df_current['link'].isin(df_ency['link'])]['link'].tolist()
    
    target_links = list(set(links_to_retry + links_to_add))
    
    if target_links:
        print(f"ìƒì„¸ ë‚´ìš© {len(target_links)}ê±´ ìˆ˜ì§‘/ê°±ì‹  ì¤‘...")
        for link in target_links:
            # df_current í˜¹ì€ df_masterì—ì„œ ì •ë³´ ì¶”ì¶œ
            source_row = df_current[df_current['link'] == link]
            if source_row.empty: source_row = df_master[df_master['link'] == link]
            if source_row.empty: continue
            
            row = source_row.iloc[0]
            content = fetch_detail_content(link)
            
            if link in df_ency['link'].values:
                df_ency.loc[df_ency['link'] == link, ['content', 'last_updated']] = [content, today]
            else:
                new_row = pd.DataFrame([{'link': link, 'company': row['company'], 'title': row['title'], 'content': content, 'last_updated': today}])
                df_ency = pd.concat([df_ency, new_row], ignore_index=True)

    # 5. íŒŒì¼ ì €ì¥ (ìµœì¢…)
    df_master.to_csv(master_file, index=False, encoding='utf-8-sig')
    df_comp.to_csv(comp_file, index=False, encoding='utf-8-sig')
    df_ency.to_csv(ency_file, index=False, encoding='utf-8-sig')
    print(f"[{today}] ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
