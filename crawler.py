import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
import re
import os

# --- ìŠ¬ë™ ì„¤ì • (GitHub Secretsì—ì„œ ê´€ë¦¬ ê¶Œì¥) ---
SLACK_WEBHOOK_URL = os.environ.get('SLACK_WEBHOOK_URL')

def get_bep_jobs():
    url = "https://bep.co.kr/Career/recruitment?type=3"
    headers = {'User-Agent': 'Mozilla/5.0'}
    jobs = []
    try:
        response = requests.get(url, headers=headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        all_links = soup.find_all('a', href=re.compile(r'recruitmentView\?idx='))
        for link_tag in all_links:
            text = link_tag.get_text(" ", strip=True)
            if "ëª¨ì§‘ì¤‘" not in text: continue
            if not any(k in text for k in ["ì „ê¸°ì°¨", "ì¶©ì „", "ì›Œí„°", "WATER"]): continue
            href = link_tag.get('href', '')
            full_link = f"https://bep.co.kr{href}" if not href.startswith('http') else href
            title = text.replace("ëª¨ì§‘ì¤‘", "").replace("ì „ê¸°ì°¨ì¶©ì „ì‚¬ì—…ë¶€ë¬¸", "").strip()
            exp = "ê³µê³  í™•ì¸"
            match = re.search(r'\(([^)]*(?:ê²½ë ¥|ì‹ ì…|ë¬´ê´€)[^)]*)\)', title)
            if match:
                exp = match.group(1)
                title = title.replace(match.group(0), "").strip()
            jobs.append(['BEP', title, exp, full_link])
    except Exception as e:
        print(f"BEP í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    return jobs

def get_saramin_jobs(companies):
    base_url = "https://www.saramin.co.kr/zf_user/search/recruit"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    jobs = []
    for company in companies:
        params = {'searchword': company, 'searchType': 'search'}
        try:
            response = requests.get(base_url, headers=headers, params=params)
            soup = BeautifulSoup(response.text, 'html.parser')
            items = soup.select('.item_recruit')
            for item in items:
                co_tag = item.select_one('.corp_name a')
                if not co_tag: continue
                co_name = co_tag.text.strip()
                if company in co_name.replace("(ì£¼)", "").replace("ì£¼ì‹íšŒì‚¬", ""):
                    title_tag = item.select_one('.job_tit a')
                    title = title_tag.text.strip()
                    link = "https://www.saramin.co.kr" + title_tag['href']
                    conds = item.select('.job_condition span')
                    exp = conds[1].text.strip() if len(conds) > 1 else "ì •ë³´ ì—†ìŒ"
                    jobs.append([co_name, title, exp, link])
            time.sleep(1)
        except Exception as e:
            print(f"ì‚¬ëŒì¸ {company} ì˜¤ë¥˜: {e}")
    return jobs

def send_slack_message(new_jobs):
    if not SLACK_WEBHOOK_URL or not new_jobs:
        return
    
    count = len(new_jobs)
    message = f"ğŸ“¢ *ì‹ ê·œ ì „ê¸°ì°¨ ì¶©ì „ ì±„ìš© ê³µê³  ({count}ê±´)*\n\n"
    for job in new_jobs:
        message += f"â€¢ *[{job[0]}]* {job[1]} ({job[2]})\n  <{job[3]}|ê³µê³  ë³´ê¸°>\n\n"
    
    payload = {"text": message}
    requests.post(SLACK_WEBHOOK_URL, json=payload)

def main():
    saramin_target = ["ëŒ€ì˜ì±„ë¹„", "ì´ë¸Œì´ì‹œìŠ¤", "í”ŒëŸ¬ê·¸ë§í¬", "ë³¼íŠ¸ì—…", "ì°¨ì§€ë¹„", "ì—ë²„ì˜¨"]
    today_str = datetime.now().strftime('%Y-%m-%d')
    
    # 1. í¬ë¡¤ë§ ìˆ˜í–‰
    print(f"[{today_str}] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    current_data = get_bep_jobs() + get_saramin_jobs(saramin_target)
    df_current = pd.DataFrame(current_data, columns=['company', 'title', 'experience', 'link'])
    
    # 2. ì „ë‚ (ê¸°ì¡´) ë°ì´í„° ë¡œë“œ ë° ë¹„êµ
    master_file = "job_listings_all.csv"
    new_jobs_list = []
    
    if os.path.exists(master_file):
        df_old = pd.read_csv(master_file)
        # ë§í¬(link)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê¸°ì¡´ì— ì—†ë˜ ê³µê³ ë§Œ ì¶”ì¶œ
        df_new = df_current[~df_current['link'].isin(df_old['link'])]
        new_jobs_list = df_new.values.tolist()
        
        # ì‹ ê·œ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ë§ˆìŠ¤í„° íŒŒì¼ ì—…ë°ì´íŠ¸
        if not df_new.empty:
            df_updated = pd.concat([df_old, df_new], ignore_index=True)
            df_updated.to_csv(master_file, index=False, encoding='utf-8-sig')
            # ì‹ ê·œ íŒŒì¼ ë³„ë„ ì €ì¥
            df_new.to_csv(f"new_jobs_{today_str}.csv", index=False, encoding='utf-8-sig')
    else:
        # ìµœì´ˆ ì‹¤í–‰ ì‹œ í˜„ì¬ ë°ì´í„°ë¥¼ ë§ˆìŠ¤í„°ë¡œ ì €ì¥
        df_current.to_csv(master_file, index=False, encoding='utf-8-sig')
        new_jobs_list = current_data
        
    # 3. ìŠ¬ë™ ì•Œë¦¼ ë°œì†¡
    if new_jobs_list:
        print(f"ì‹ ê·œ ê³µê³  {len(new_jobs_list)}ê±´ ë°œê²¬! ìŠ¬ë™ ì „ì†¡ ì¤‘...")
        send_slack_message(new_jobs_list)
    else:
        print("ì‹ ê·œ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
