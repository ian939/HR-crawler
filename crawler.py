import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
import re
import os

# --- í™˜ê²½ ì„¤ì • ---
SLACK_WEBHOOK_URL = os.environ.get('SLACK_WEBHOOK_URL')
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
}

def fetch_detail_content(url):
    """ìƒì„¸ í˜ì´ì§€ ë³¸ë¬¸ ì¶”ì¶œ (ì‚¬ëŒì¸ íŠ¹ìˆ˜ ì£¼ì†Œ ëŒ€ì‘ ë° ë…¸ì´ì¦ˆ í•„í„°ë§)"""
    try:
        time.sleep(2)
        target_url = url
        
        # [ì‚¬ëŒì¸ ì „ìš©] ìƒì„¸ ìš”ê°•ì´ ë“¤ì–´ìˆëŠ” ì‹¤ì œ ë°ì´í„° URLë¡œ ìš°íšŒ (ë¡œê·¸ì¸/ìš”ì•½ë¬¸êµ¬ íšŒí”¼)
        if "saramin.co.kr" in url and "rec_idx=" in url:
            rec_idx_match = re.search(r'rec_idx=(\d+)', url)
            if rec_idx_match:
                # view-detail ì£¼ì†Œê°€ ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë‹´ê³  ìˆëŠ” ê²½ìš°ê°€ ë§ìŒ
                target_url = f"https://www.saramin.co.kr/zf_user/jobs/relay/view-detail?rec_idx={rec_idx_match.group(1)}"

        res = requests.get(target_url, headers=HEADERS, timeout=15)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')

        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(["script", "style", "nav", "footer", "header", "button", "aside"]):
            tag.decompose()

        # ë³¸ë¬¸ì´ ìœ„ì¹˜í•˜ëŠ” ì£¼ìš” í´ë˜ìŠ¤ë“¤ (ì—ë²„ì˜¨ì²˜ëŸ¼ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ ì˜ì—­ ì°¾ê¸°)
        selectors = [
            '.user_content',        # ì‚¬ëŒì¸ ë³¸ë¬¸ ì˜ì—­
            '.recruit_view_cont',   # BEP ë³¸ë¬¸ ì˜ì—­
            '.view_con',            # ì¼ë°˜ì ì¸ ë³¸ë¬¸ 1
            '.job_detail',          # ì¼ë°˜ì ì¸ ë³¸ë¬¸ 2
            '.template_area',       # ì‚¬ëŒì¸ í…œí”Œë¦¿ ì˜ì—­
            '#content'              # ê¸°ë³¸ ì•„ì´ë””
        ]
        
        content_text = ""
        for sel in selectors:
            target = soup.select_one(sel)
            if target:
                candidate = target.get_text(separator="\n", strip=True)
                # "ì±„ìš©ê³µê³  ìƒì„¸" ê°™ì€ ì§§ì€ ë¬¸êµ¬ëŠ” ë¬´ì‹œí•˜ê³  ì˜ë¯¸ ìˆëŠ” ê¸¸ì´(50ì ì´ìƒ)ë§Œ ì„ íƒ
                if len(candidate) > 50 and "ì±„ìš©ê³µê³  ìƒì„¸" not in candidate[:15]:
                    content_text = candidate
                    break
        
        # ì—¬ì „íˆ ë‚´ìš©ì„ ëª» ì°¾ì•˜ë‹¤ë©´, ê°€ì¥ í…ìŠ¤íŠ¸ê°€ ë§ì€ ì˜ì—­ì„ ì¶”ì¶œ ì‹œë„
        if len(content_text) < 100:
            all_text = soup.get_text(separator="\n", strip=True)
            # ë¡œê·¸ì¸ ê´€ë ¨ ë…¸ì´ì¦ˆ ì œê±°
            if "ë¡œê·¸ì¸" in all_text[:200] and len(all_text) < 500:
                return "ë³¸ë¬¸ ë‚´ìš© í™•ì¸ ë¶ˆê°€ (ìƒì„¸ í˜ì´ì§€ ë§í¬ ì°¸ì¡°)"
            content_text = all_text

        return content_text[:20000] # CSV ì €ì¥ í•œê³„ ê³ ë ¤
    except Exception as e:
        return f"ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"

def get_bep_jobs():
    """BEP ìˆ˜ì§‘ ë¡œì§ - ì›Œí„°(ì „ê¸°ì°¨ì¶©ì „ì‚¬ì—…ë¶€ë¬¸) ì „ìš© í˜ì´ì§€ í¬ë¡¤ë§"""
    # ì‚¬ìš©ìê°€ ì§€ì •í•œ type=3 (ì „ê¸°ì°¨ì¶©ì „ì‚¬ì—…ë¶€ë¬¸) í•„í„° ì ìš© URL
    url = "https://bep.co.kr/Career/recruitment?type=3"
    jobs = []
    try:
        response = requests.get(url, headers=HEADERS)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # BEP ì±„ìš© ëª©ë¡ ë§í¬ ì¶”ì¶œ
        links = soup.find_all('a', href=re.compile(r'recruitmentView\?idx='))
        for l in links:
            title = l.get_text(" ", strip=True)
            # ëª©ë¡ ì´ë™ ë²„íŠ¼ ë“± ë¶ˆí•„ìš”í•œ ë§í¬ ì œì™¸
            if not title or "ëª©ë¡" in title: continue
            
            href = l.get('href', '')
            full_link = f"https://bep.co.kr{href}" if not href.startswith('http') else href
            
            # ì œëª©ì—ì„œ 'ëª¨ì§‘ì¤‘' í‚¤ì›Œë“œ ì œê±°
            clean_title = title.replace("ëª¨ì§‘ì¤‘", "").strip()
            
            # ì´ í˜ì´ì§€ëŠ” ì´ë¯¸ í•„í„°ë§ëœ í˜ì´ì§€ì´ë¯€ë¡œ ë°”ë¡œ ì¶”ê°€
            jobs.append(['BEP(ì›Œí„°)', clean_title, "ê³µê³  ì°¸ì¡°", full_link])
    except Exception as e:
        print(f"BEP í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    return jobs

def get_saramin_jobs(companies):
    """ì‚¬ëŒì¸ ê¸°ì—… ê²€ìƒ‰ ë° ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ"""
    base_url = "https://www.saramin.co.kr/zf_user/search/recruit"
    jobs = []
    for company in companies:
        try:
            params = {'searchword': company, 'searchType': 'search'}
            res = requests.get(base_url, headers=HEADERS, params=params)
            soup = BeautifulSoup(res.text, 'html.parser')
            items = soup.select('.item_recruit')
            for item in items:
                co_tag = item.select_one('.corp_name a')
                if not co_tag: continue
                co_name = co_tag.text.strip()
                # ê¸°ì—…ëª… í•„í„°ë§ (ì£¼ì‹íšŒì‚¬ ë“± ì œì™¸ ë§¤ì¹­)
                if company in co_name.replace("(ì£¼)", "").replace("ì£¼ì‹íšŒì‚¬", ""):
                    title_tag = item.select_one('.job_tit a')
                    link = "https://www.saramin.co.kr" + title_tag['href']
                    jobs.append([co_name, title_tag.text.strip(), "ê³µê³  ì°¸ì¡°", link])
            time.sleep(1.5)
        except: continue
    return jobs

def safe_load_df(file_path, default_cols):
    """íŒŒì¼ ë¡œë“œ ì‹œ ì»¬ëŸ¼ëª… ì •ì œ ë¡œì§"""
    if os.path.exists(file_path):
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            df.columns = [c.strip().replace('\ufeff', '') for c in df.columns]
            for col in default_cols:
                if col not in df.columns: df[col] = ""
            return df[default_cols]
        except:
            return pd.DataFrame(columns=default_cols)
    return pd.DataFrame(columns=default_cols)

def main():
    saramin_targets = ["ëŒ€ì˜ì±„ë¹„", "ì´ë¸Œì´ì‹œìŠ¤", "í”ŒëŸ¬ê·¸ë§í¬", "ë³¼íŠ¸ì—…", "ì°¨ì§€ë¹„", "ì—ë²„ì˜¨"]
    today = datetime.now().strftime('%Y-%m-%d')
    
    # ë°ì´í„° ë¡œë“œ
    df_master = safe_load_df("job_listings_all.csv", ['company', 'title', 'experience', 'link', 'first_seen'])
    df_ency = safe_load_df("encyclopedia.csv", ['link', 'company', 'title', 'content', 'last_updated'])
    df_comp = safe_load_df("Recruitment_completed.csv", ['company', 'title', 'experience', 'link', 'completed_date'])

    print(f"[{today}] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    
    # 1. ìˆ˜ì§‘ ìˆ˜í–‰ (BEPëŠ” íŠ¹ì • URL ì‚¬ìš©)
    current_jobs = get_bep_jobs() + get_saramin_jobs(saramin_targets)
    df_current = pd.DataFrame(current_jobs, columns=['company', 'title', 'experience', 'link'])

    if df_current.empty:
        print("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ì‹ ê·œ ê³µê³  ì•Œë¦¼ ë° ë§ˆìŠ¤í„° ì—…ë°ì´íŠ¸
    new_entries = df_current[~df_current['link'].isin(df_master['link'])].copy()
    if not new_entries.empty:
        new_entries['first_seen'] = today
        if SLACK_WEBHOOK_URL:
            msg = f"ğŸ“¢ *ì‹ ê·œ ì±„ìš© ({len(new_entries)}ê±´)*\n"
            for _, r in new_entries.iterrows(): msg += f"â€¢ [{r['company']}] {r['title']}\n  <{r['link']}|ê³µê³  ë³´ê¸°>\n"
            requests.post(SLACK_WEBHOOK_URL, json={"text": msg})
        df_master = pd.concat([df_master, new_entries], ignore_index=True)

    # 3. ì±„ìš© ì¢…ë£Œ ì²˜ë¦¬
    successful_scan_cos = df_current['company'].unique()
    is_missing = ~df_master['link'].isin(df_current['link'])
    is_safe = df_master['company'].isin(successful_scan_cos)
    closed_entries = df_master[is_missing & is_safe].copy()
    if not closed_entries.empty:
        closed_entries['completed_date'] = today
        df_comp = pd.concat([df_comp, closed_entries], ignore_index=True)
        df_master = df_master[~(is_missing & is_safe)]

    # 4. ë°±ê³¼ì‚¬ì „(Encyclopedia) ë³¸ë¬¸ ìˆ˜ì§‘
    # ëŒ€ìƒ: ë°±ê³¼ì‚¬ì „ì— ì•„ì˜ˆ ì—†ê±°ë‚˜, ê¸°ì¡´ ë‚´ìš©ì´ 'ì±„ìš©ê³µê³  ìƒì„¸' ë˜ëŠ” ë¶€ì‹¤í•œ ê²½ìš°
    retry_keywords = ["ì±„ìš©ê³µê³  ìƒì„¸", "ë³¸ë¬¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ë¡œê·¸ì¸", "í™•ì¸ ë¶ˆê°€", "ë§í¬ ì°¸ì¡°"]
    is_poor_content = df_ency['content'].apply(lambda x: any(k in str(x) for k in retry_keywords) or len(str(x)) < 100)
    
    retry_links = df_ency[is_poor_content]['link'].tolist() if not df_ency.empty else []
    add_links = df_current[~df_current['link'].isin(df_ency['link'])]['link'].tolist()
    target_links = list(set(retry_links + add_links))

    if target_links:
        print(f"ìƒì„¸ ë³¸ë¬¸ {len(target_links)}ê±´ ìˆ˜ì§‘/ê°±ì‹  ì¤‘...")
        for link in target_links:
            source = df_current[df_current['link'] == link]
            if source.empty: source = df_master[df_master['link'] == link]
            if source.empty: continue
            
            row = source.iloc[0]
            content = fetch_detail_content(link)
            
            if link in df_ency['link'].values:
                df_ency.loc[df_ency['link'] == link, ['content', 'last_updated']] = [content, today]
            else:
                new_row = pd.DataFrame([{'link': link, 'company': row['company'], 'title': row['title'], 'content': content, 'last_updated': today}])
                df_ency = pd.concat([df_ency, new_row], ignore_index=True)

    # íŒŒì¼ ì €ì¥
    df_master.to_csv("job_listings_all.csv", index=False, encoding='utf-8-sig')
    df_comp.to_csv("Recruitment_completed.csv", index=False, encoding='utf-8-sig')
    df_ency.to_csv("encyclopedia.csv", index=False, encoding='utf-8-sig')
    print("ì„±ê³µì ìœ¼ë¡œ ì‘ì—…ì„ ë§ˆì³¤ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
