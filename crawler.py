import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
import re
import os
from urllib.parse import urljoin

# --- í™˜ê²½ ì„¤ì • ---
SLACK_WEBHOOK_URL = os.environ.get('SLACK_WEBHOOK_URL')
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
}

def safe_load_df(file_path, default_cols):
    """íŒŒì¼ ë¡œë“œ ì‹œ ì»¬ëŸ¼ëª…ì„ ê°•ì œí•˜ê³  ì¤‘ë³µ ì œê±°"""
    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
        try:
            # quoting=1 (QUOTE_ALL) ë“±ì„ ê³ ë ¤í•˜ì—¬ ìœ ì—°í•˜ê²Œ ì½ìŒ
            df = pd.read_csv(file_path, encoding='utf-8-sig', on_bad_lines='skip')
            df.columns = [c.strip().replace('\ufeff', '') for c in df.columns]
            
            # link ì»¬ëŸ¼ì´ ì•„ì˜ˆ ì—†ê±°ë‚˜ ê¹¨ì§„ ê²½ìš° ëŒ€ë¹„
            if 'link' not in df.columns:
                print(f"ê²½ê³ : {file_path}ì— 'link' ì»¬ëŸ¼ì´ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                df = pd.DataFrame(columns=default_cols)
            
            for col in default_cols:
                if col not in df.columns: df[col] = ""
            return df[default_cols].drop_duplicates(subset=['link'])
        except Exception as e:
            print(f"ë¡œë“œ ì‹¤íŒ¨({file_path}): {e}")
            return pd.DataFrame(columns=default_cols)
    return pd.DataFrame(columns=default_cols)

def fetch_detail_content(url):
    """ìƒì„¸ ë³¸ë¬¸ ì¶”ì¶œ (ì´ë¯¸ì§€ í¬í•¨)"""
    try:
        time.sleep(1.2)
        target_url = url
        if "saramin.co.kr" in url and "rec_idx=" in url:
            rec_idx_match = re.search(r'rec_idx=(\d+)', url)
            if rec_idx_match:
                target_url = f"https://www.saramin.co.kr/zf_user/jobs/relay/view-detail?rec_idx={rec_idx_match.group(1)}"

        res = requests.get(target_url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        for tag in soup(["script", "style", "nav", "footer", "header", "button"]): tag.decompose()

        content_area = soup.select_one('.user_content, .recruit_view_cont, .view_con, body')
        text_content = content_area.get_text(separator="\n", strip=True) if content_area else ""
        
        if len(text_content) < 150 and content_area:
            imgs = content_area.find_all('img')
            img_urls = [urljoin(url, i.get('src') or i.get('data-src')) for i in imgs if i.get('src') or i.get('data-src')]
            clean_imgs = [i for i in img_urls if not any(x in i.lower() for x in ["icon", "logo", "common"])]
            if clean_imgs: return "[ì´ë¯¸ì§€ ê³µê³ ] " + ", ".join(clean_imgs)

        return text_content[:15000] if len(text_content) > 50 else "ìƒì„¸ ë§í¬ ì°¸ì¡°"
    except: return "ìˆ˜ì§‘ ì‹¤íŒ¨"

def get_bep_jobs():
    """BEP(ì›Œí„°) ìˆ˜ì§‘ ë¡œì§ ì „ë©´ ìˆ˜ì • (í‚¤ì›Œë“œ ê¸°ë°˜)"""
    # í•„í„°ê°€ ì ìš©ëœ URLê³¼ ì „ì²´ URL ëª¨ë‘ ì‹œë„
    search_urls = [
        "https://bep.co.kr/Career/recruitment?type=3",
        "https://bep.co.kr/Career/recruitment"
    ]
    jobs = []
    seen_links = set()
    
    for url in search_urls:
        try:
            res = requests.get(url, headers=HEADERS, timeout=15)
            soup = BeautifulSoup(res.text, 'html.parser')
            # ëª¨ë“  ê³µê³  ë§í¬(recruitmentView í¬í•¨)ë¥¼ ì°¾ìŒ
            links = soup.find_all('a', href=re.compile(r'recruitmentView'))
            
            for l in links:
                href = l.get('href')
                full_link = urljoin("https://bep.co.kr", href)
                if full_link in seen_links: continue
                
                # í•´ë‹¹ ë§í¬ì˜ í…ìŠ¤íŠ¸ì™€ ë¶€ëª¨ ìš”ì†Œì˜ ì „ì²´ í…ìŠ¤íŠ¸ í™•ì¸
                title_text = l.get_text(" ", strip=True)
                container = l.find_parent(['li', 'div', 'tr', 'td'])
                context_text = container.get_text(" ", strip=True) if container else title_text
                
                # 'ì „ê¸°ì°¨ì¶©ì „' ë˜ëŠ” 'ì›Œí„°' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°ë§Œ ìˆ˜ì§‘
                if any(k in context_text for k in ["ì „ê¸°ì°¨ì¶©ì „", "ì›Œí„°", "EV"]):
                    clean_title = title_text.replace("ëª¨ì§‘ì¤‘", "").strip()
                    if not clean_title or clean_title in ["ëª©ë¡", "ì´ì „", "ë‹¤ìŒ"]: continue
                    
                    jobs.append(['BEP(ì›Œí„°)', clean_title, "ê³µê³  í™•ì¸", full_link])
                    seen_links.add(full_link)
            if jobs: break # ë°ì´í„°ë¥¼ ì°¾ì•˜ìœ¼ë©´ ë‹¤ìŒ URL ì‹œë„ ì•ˆ í•¨
        except: continue
    return jobs

def get_saramin_jobs(companies):
    """ì‚¬ëŒì¸ ìˆ˜ì§‘"""
    base_url = "https://www.saramin.co.kr/zf_user/search/recruit"
    jobs = []
    for company in companies:
        try:
            params = {'searchword': company, 'searchType': 'search'}
            res = requests.get(base_url, headers=HEADERS, params=params, timeout=15)
            soup = BeautifulSoup(res.text, 'html.parser')
            for item in soup.select('.item_recruit'):
                co_tag = item.select_one('.corp_name a')
                if co_tag and company in co_tag.text:
                    title_tag = item.select_one('.job_tit a')
                    conds = item.select('.job_condition span')
                    exp = conds[1].text.strip() if len(conds) > 1 else "ê²½ë ¥ë¬´ê´€"
                    jobs.append([co_tag.text.strip(), title_tag.text.strip(), exp, "https://www.saramin.co.kr" + title_tag['href']])
            time.sleep(1)
        except: continue
    return jobs

def main():
    today = datetime.now().strftime('%Y-%m-%d')
    
    # 1. ë°ì´í„° ë¡œë“œ
    df_master = safe_load_df("job_listings_all.csv", ['company', 'title', 'experience', 'link', 'first_seen'])
    df_ency = safe_load_df("encyclopedia.csv", ['link', 'company', 'title', 'content', 'last_updated'])
    df_comp = safe_load_df("Recruitment_completed.csv", ['company', 'title', 'experience', 'link', 'completed_date'])

    print(f"[{today}] ìˆ˜ì§‘ ì‹œì‘...")
    scraped = get_bep_jobs() + get_saramin_jobs(["ëŒ€ì˜ì±„ë¹„", "ì´ë¸Œì´ì‹œìŠ¤", "í”ŒëŸ¬ê·¸ë§í¬", "ë³¼íŠ¸ì—…", "ì°¨ì§€ë¹„", "ì—ë²„ì˜¨"])
    df_current = pd.DataFrame(scraped, columns=['company', 'title', 'experience', 'link']).drop_duplicates(subset=['link'])

    if df_current.empty:
        print("ìˆ˜ì§‘ëœ ì‹ ê·œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # 2. ì‹ ê·œ ì•Œë¦¼ ë° ë³‘í•©
        new_entries = df_current[~df_current['link'].isin(df_master['link'])].copy()
        if not new_entries.empty:
            new_entries['first_seen'] = today
            if SLACK_WEBHOOK_URL:
                msg = f"ğŸ“¢ *ì‹ ê·œ ì±„ìš© ({len(new_entries)}ê±´)*\n"
                for _, r in new_entries.iterrows():
                    msg += f"â€¢ [{r['company']}] {r['title']}\n  <{r['link']}|ë³´ê¸°>\n"
                requests.post(SLACK_WEBHOOK_URL, json={"text": msg})
            df_master = pd.concat([df_master, new_entries], ignore_index=True)

        # 3. ì±„ìš© ì¢…ë£Œ ì²˜ë¦¬
        active_links = df_current['link'].tolist()
        is_closed = (~df_master['link'].isin(active_links)) & (df_master['company'].isin(df_current['company'].unique()))
        closed_jobs = df_master[is_closed].copy()
        if not closed_jobs.empty:
            closed_jobs['completed_date'] = today
            df_comp = pd.concat([df_comp, closed_jobs], ignore_index=True).drop_duplicates(subset=['link'])
            df_master = df_master[~is_closed]

    # 4. Encyclopedia ì—…ë°ì´íŠ¸ ë° ì •ë ¬
    if 'link' in df_ency.columns and not df_master.empty:
        retry_keywords = ["ìˆ˜ì§‘ ì‹¤íŒ¨", "ë¡œê·¸ì¸", "ìƒì„¸ ë§í¬ ì°¸ì¡°"]
        is_bad = df_ency['content'].fillna("").apply(lambda x: any(k in str(x) for k in retry_keywords) or len(str(x)) < 150)
        
        target_links = df_ency[is_bad]['link'].tolist() + df_master[~df_master['link'].isin(df_ency['link'])]['link'].tolist()
        target_links = list(set(target_links))

        if target_links:
            print(f"ìƒì„¸ ìˆ˜ì§‘/ì—…ë°ì´íŠ¸ ì¤‘... ({len(target_links)}ê±´)")
            for link in target_links:
                info = df_master[df_master['link'] == link]
                if info.empty: continue
                content = fetch_detail_content(link)
                if link in df_ency['link'].values:
                    df_ency.loc[df_ency['link'] == link, ['content', 'last_updated']] = [content, today]
                else:
                    new_row = pd.DataFrame([{'link': link, 'company': info.iloc[0]['company'], 'title': info.iloc[0]['title'], 'content': content, 'last_updated': today}])
                    df_ency = pd.concat([df_ency, new_row], ignore_index=True)

    # 5. ìµœì¢… ì •ë ¬ (íšŒì‚¬ëª… ë‚´ë¦¼ì°¨ìˆœ) ë° ì €ì¥
    if 'company' in df_ency.columns:
        df_ency = df_ency.sort_values(by='company', ascending=False)

    df_master.to_csv("job_listings_all.csv", index=False, encoding='utf-8-sig')
    df_comp.to_csv("Recruitment_completed.csv", index=False, encoding='utf-8-sig')
    df_ency.to_csv("encyclopedia.csv", index=False, encoding='utf-8-sig')
    print(f"ì‘ì—… ì™„ë£Œ. (í˜„ì¬ ê³µê³ : {len(df_master)}ê±´, ë°±ê³¼ì‚¬ì „: {len(df_ency)}ê±´)")

if __name__ == "__main__":
    main()
