import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
import re
import os
from urllib.parse import urljoin, urlparse, parse_qs

# --- í™˜ê²½ ì„¤ì • ---
SLACK_WEBHOOK_URL = os.environ.get('SLACK_WEBHOOK_URL')
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

def safe_load_df(file_path, default_cols):
    """íŒŒì¼ ë¡œë“œ ì‹œ ì»¬ëŸ¼ ë³´ì¥ ë° ì¤‘ë³µ/ê³µë°± ì œê±°"""
    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig', on_bad_lines='skip')
            df.columns = [c.strip().replace('\ufeff', '') for c in df.columns]
            # link ì»¬ëŸ¼ ë°ì´í„° ì •ì œ ë° ì¤‘ë³µ ì œê±°
            if 'link' in df.columns:
                df['link'] = df['link'].astype(str).str.strip()
                df = df.drop_duplicates(subset=['link'], keep='first')
            # ì—†ëŠ” ì»¬ëŸ¼ ìƒì„±
            for col in default_cols:
                if col not in df.columns: 
                    df[col] = "" if col not in ['first_seen', 'completed_date', 'last_updated'] else None
            return df[default_cols]
        except Exception as e:
            print(f"ë¡œë“œ ì‹¤íŒ¨({file_path}): {e}")
            return pd.DataFrame(columns=default_cols)
    return pd.DataFrame(columns=default_cols)

def is_invalid_content(text):
    """ìœ íš¨í•˜ì§€ ì•Šì€ content íŒë³„"""
    if not text or len(text) < 50:
        return True
    
    invalid_keywords = [
        "ë¡œê·¸ì¸ì´ í•„ìš”í•œ",
        "ë¡œê·¸ì¸ ìœ ì§€",
        "ì•„ì´ë”” ë¹„ë°€ë²ˆí˜¸",
        "íšŒì›ê°€ì…",
        "ë³¸ë¬¸ ë°”ë¡œê°€ê¸°",
        "ê²€ìƒ‰ í¼",
        "ê°œì¸ì •ë³´ ë³´í˜¸",
        "ì†Œì…œ ê³„ì •ìœ¼ë¡œ",
        "ì±„ìš©ê³¼ì •ì—ì„œ ìˆ˜ì§‘ëœ"
    ]
    
    # 3ê°œ ì´ìƒì˜ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ íŒë‹¨
    keyword_count = sum(1 for kw in invalid_keywords if kw in text)
    if keyword_count >= 3:
        return True
    
    return False

def extract_saramin_detail(url):
    """ì‚¬ëŒì¸ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ (ê°œì„  ë²„ì „)"""
    try:
        # rec_idx ì¶”ì¶œ
        parsed = urlparse(url)
        params = parse_qs(parsed.query)
        rec_idx = params.get('rec_idx', [None])[0]
        
        if not rec_idx:
            return "ë§í¬ ì˜¤ë¥˜"
        
        # ì§ì ‘ ìƒì„¸ í˜ì´ì§€ë¡œ ì ‘ê·¼
        detail_url = f"https://www.saramin.co.kr/zf_user/jobs/relay/view?rec_idx={rec_idx}"
        
        session = requests.Session()
        session.headers.update(HEADERS)
        
        time.sleep(1)
        res = session.get(detail_url, timeout=15, allow_redirects=True)
        
        # ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if 'member/login' in res.url or res.status_code != 200:
            print(f"    âš ï¸  ë¡œê·¸ì¸ í•„ìš” - ì´ë¯¸ì§€ ê³µê³  íƒìƒ‰")
            # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œë„
            return extract_image_from_search(url)
        
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # ë¶ˆí•„ìš” ìš”ì†Œ ì œê±°
        for tag in soup.select('script, style, nav, footer, header, .btn_area, .login_wrap, #gfm_frame'):
            tag.decompose()
        
        # 1ìˆœìœ„: ì±„ìš© ê³µê³  ë³¸ë¬¸
        content_selectors = [
            '.user_content',
            '.jobcont_wrap',
            '.jv_cont',
            '#content',
            '.recruit_contents'
        ]
        
        for selector in content_selectors:
            content_area = soup.select_one(selector)
            if content_area:
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = content_area.get_text(separator="\n", strip=True)
                
                # ìœ íš¨ì„± ê²€ì‚¬
                if not is_invalid_content(text) and len(text) > 100:
                    # ì—°ì† ê³µë°± ë° ì¤„ë°”ê¿ˆ ì •ë¦¬
                    text = re.sub(r'\n{3,}', '\n\n', text)
                    text = re.sub(r' {2,}', ' ', text)
                    return text[:15000]
        
        # 2ìˆœìœ„: ì´ë¯¸ì§€ ê³µê³  ì¶”ì¶œ
        return extract_image_from_search(url)
        
    except Exception as e:
        print(f"    âœ— ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return extract_image_from_search(url)

def extract_image_from_search(url):
    """ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
    try:
        parsed = urlparse(url)
        params = parse_qs(parsed.query)
        rec_idx = params.get('rec_idx', [None])[0]
        searchword = params.get('searchword', [''])[0]
        
        if not rec_idx:
            return "ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨"
        
        # ê²€ìƒ‰ í˜ì´ì§€ ì ‘ê·¼
        search_url = f"https://www.saramin.co.kr/zf_user/search/recruit?searchword={searchword}"
        
        time.sleep(0.5)
        res = requests.get(search_url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # í•´ë‹¹ rec_idxë¥¼ ê°€ì§„ í•­ëª© ì°¾ê¸°
        for item in soup.select('.item_recruit'):
            link_tag = item.select_one('.job_tit a')
            if link_tag and rec_idx in link_tag.get('href', ''):
                # ì´ë¯¸ì§€ íƒœê·¸ ì°¾ê¸°
                img_tag = item.select_one('.logo img, .thumb img, img')
                if img_tag:
                    img_src = img_tag.get('src') or img_tag.get('data-src')
                    if img_src and 'recruit' in img_src:
                        return f"[ì´ë¯¸ì§€ ê³µê³ ] {img_src}"
        
        # ì§ì ‘ ì ‘ê·¼ ì‹œë„
        detail_url = f"https://www.saramin.co.kr/zf_user/jobs/relay/view?rec_idx={rec_idx}"
        res = requests.get(detail_url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        imgs = soup.select('.user_content img, .jobcont_wrap img')
        recruit_imgs = [img.get('src') or img.get('data-src') for img in imgs 
                       if img.get('src') and 'recruit' in img.get('src', '')]
        
        if recruit_imgs:
            return "[ì´ë¯¸ì§€ ê³µê³ ] " + ", ".join(recruit_imgs[:2])
        
        return "ìƒì„¸ ë§í¬ ì°¸ì¡°"
        
    except:
        return "ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨"

def fetch_detail_content(url):
    """ìƒì„¸ ë³¸ë¬¸ ì¶”ì¶œ (ì‚¬ì´íŠ¸ë³„ ë¶„ê¸°)"""
    try:
        # ì‚¬ëŒì¸
        if 'saramin.co.kr' in url:
            return extract_saramin_detail(url)
        
        # BEP ë˜ëŠ” ê¸°íƒ€
        time.sleep(1)
        res = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # ë¶ˆí•„ìš” íƒœê·¸ ì œê±°
        for tag in soup(["script", "style", "nav", "footer", "header", "button"]): 
            tag.decompose()

        # ë³¸ë¬¸ ì˜ì—­ íƒìƒ‰
        content_area = soup.select_one('.user_content, .recruit_view_cont, .view_con, .job_detail, .content, body')
        text_content = content_area.get_text(separator="\n", strip=True) if content_area else ""
        
        # ìœ íš¨ì„± ê²€ì‚¬
        if is_invalid_content(text_content):
            text_content = ""
        
        # í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ë©´ ì´ë¯¸ì§€ ìˆ˜ì§‘
        if len(text_content) < 150 and content_area:
            imgs = content_area.find_all('img')
            img_urls = [urljoin(url, i.get('src') or i.get('data-src')) 
                       for i in imgs if i.get('src') or i.get('data-src')]
            clean_imgs = [i for i in img_urls 
                         if not any(x in i.lower() for x in ["icon", "logo", "common", "header"])]
            
            if clean_imgs: 
                return "[ì´ë¯¸ì§€ ê³µê³ ] " + ", ".join(clean_imgs[:3])

        return text_content[:15000] if len(text_content) > 50 else "ìƒì„¸ ë§í¬ ì°¸ì¡°"
        
    except Exception as e:
        print(f"    [ìƒì„¸ìˆ˜ì§‘ ì‹¤íŒ¨] {str(e)[:50]}")
        return "ìˆ˜ì§‘ ì‹¤íŒ¨"

def get_bep_jobs():
    """BEP(ì›Œí„°) ìˆ˜ì§‘: Selenium ì—†ì´ ìµœëŒ€í•œ ìš°íšŒ"""
    url = "https://bep.co.kr/Career/recruitment?type=3"
    jobs = []
    
    try:
        print("  [BEP ìˆ˜ì§‘ ì‹œì‘]")
        
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://bep.co.kr/Career',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        })
        
        # ë©”ì¸ í˜ì´ì§€ ë¨¼ì € ë°©ë¬¸ (ì¿ í‚¤ íšë“)
        session.get('https://bep.co.kr/', timeout=10)
        time.sleep(1)
        
        # ì±„ìš© í˜ì´ì§€ ì ‘ê·¼
        res = session.get(url, timeout=20)
        res.raise_for_status()
        
        soup = BeautifulSoup(res.text, 'html.parser')
        print(f"  [ì‘ë‹µì½”ë“œ] {res.status_code}")
        
        # ì „ëµ 1: í…Œì´ë¸” êµ¬ì¡° íƒìƒ‰
        table = soup.find('table') or soup.find('tbody')
        if table:
            rows = table.find_all('tr')
            print(f"  [í…Œì´ë¸”] {len(rows)}ê°œ í–‰ ë°œê²¬")
            
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) < 2:
                    continue
                
                # ë§í¬ ì°¾ê¸°
                link_tag = row.find('a', href=True)
                if not link_tag:
                    continue
                
                href = link_tag.get('href', '').strip()
                if not href or href in ['#', 'javascript:void(0)']:
                    continue
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                title_text = link_tag.get_text(strip=True)
                
                # ìƒíƒœ í™•ì¸ (ëª¨ì§‘ì¤‘ì¸ì§€)
                row_text = row.get_text()
                if "ë§ˆê°" in row_text or "ì¢…ë£Œ" in row_text:
                    continue
                
                # í•„í„°ë§
                if not title_text or len(title_text) < 3:
                    continue
                if title_text in ["ëª©ë¡", "ì´ì „", "ë‹¤ìŒ", "HOME"]:
                    continue
                
                full_link = urljoin("https://bep.co.kr", href)
                
                # ì¤‘ë³µ ì²´í¬
                if any(j[3] == full_link for j in jobs):
                    continue
                
                print(f"    âœ“ {title_text}")
                jobs.append(['BEP(ì›Œí„°)', title_text, "ê³µê³  í™•ì¸", full_link])
        
        # ì „ëµ 2: ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°
        if not jobs:
            items = soup.select('.recruit_list li, .list_item, .recruitment_item')
            print(f"  [ë¦¬ìŠ¤íŠ¸] {len(items)}ê°œ í•­ëª© íƒìƒ‰")
            
            for item in items:
                link_tag = item.find('a', href=True)
                if not link_tag:
                    continue
                
                href = link_tag.get('href', '').strip()
                title = link_tag.get_text(strip=True)
                
                if href and title and len(title) > 3:
                    full_link = urljoin("https://bep.co.kr", href)
                    if not any(j[3] == full_link for j in jobs):
                        jobs.append(['BEP(ì›Œí„°)', title, "ê³µê³  í™•ì¸", full_link])
        
        # ì „ëµ 3: recruitmentView ì§ì ‘ ê²€ìƒ‰
        if not jobs:
            all_links = soup.find_all('a', href=re.compile(r'recruitment'))
            print(f"  [ì „ì²´ë§í¬] {len(all_links)}ê°œ í•„í„°ë§")
            
            for link in all_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                if 'view' in href.lower() and len(text) > 3:
                    if text not in ["ëª©ë¡", "ì´ì „", "ë‹¤ìŒ"]:
                        full_link = urljoin("https://bep.co.kr", href)
                        if not any(j[3] == full_link for j in jobs):
                            jobs.append(['BEP(ì›Œí„°)', text, "ê³µê³  í™•ì¸", full_link])
        
        print(f"  [BEP ì™„ë£Œ] {len(jobs)}ê±´ ìˆ˜ì§‘")
        
        # ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ì§ì ‘ ë§í¬ ì¶”ê°€ (ì•Œë ¤ì§„ ê³µê³ ê°€ ìˆë‹¤ë©´)
        if not jobs:
            print("  âš ï¸  ìë™ ìˆ˜ì§‘ ì‹¤íŒ¨ - ìˆ˜ë™ í™•ì¸ í•„ìš”")
        
    except Exception as e:
        print(f"  [BEP ì˜¤ë¥˜] {e}")
    
    return jobs

def get_saramin_jobs(companies):
    """ì‚¬ëŒì¸ ìˆ˜ì§‘"""
    base_url = "https://www.saramin.co.kr/zf_user/search/recruit"
    jobs = []
    
    for company in companies:
        try:
            print(f"  [ì‚¬ëŒì¸] {company} ê²€ìƒ‰ ì¤‘...")
            params = {'searchword': company, 'searchType': 'search'}
            res = requests.get(base_url, headers=HEADERS, params=params, timeout=15)
            soup = BeautifulSoup(res.text, 'html.parser')
            
            count = 0
            for item in soup.select('.item_recruit'):
                co_tag = item.select_one('.corp_name a')
                if co_tag and company in co_tag.text.replace(" ", ""):
                    title_tag = item.select_one('.job_tit a')
                    conds = item.select('.job_condition span')
                    exp = conds[1].text.strip() if len(conds) > 1 else "ê²½ë ¥ë¬´ê´€"
                    
                    link = "https://www.saramin.co.kr" + title_tag['href']
                    jobs.append([co_tag.text.strip(), title_tag.text.strip(), exp, link])
                    count += 1
            
            print(f"    âœ“ {count}ê±´ ë°œê²¬")
            time.sleep(1)
            
        except Exception as e:
            print(f"    âœ— ì˜¤ë¥˜: {e}")
            continue
    
    return jobs

def main():
    today = datetime.now().strftime('%Y-%m-%d')
    
    # 1. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    df_master = safe_load_df("job_listings_all.csv", ['company', 'title', 'experience', 'link', 'first_seen'])
    df_ency = safe_load_df("encyclopedia.csv", ['link', 'company', 'title', 'content', 'first_seen', 'completed_date', 'last_updated'])
    df_comp = safe_load_df("Recruitment_completed.csv", ['company', 'title', 'experience', 'link', 'completed_date', 'first_seen'])

    print(f"\n{'='*60}")
    print(f"[{today}] ì±„ìš©ê³µê³  ìˆ˜ì§‘ ì‹œì‘")
    print(f"{'='*60}\n")
    
    # 2. í¬ë¡¤ë§ ì‹¤í–‰
    targets = ["ëŒ€ì˜ì±„ë¹„", "ì´ë¸Œì´ì‹œìŠ¤", "í”ŒëŸ¬ê·¸ë§í¬", "ë³¼íŠ¸ì—…", "ì°¨ì§€ë¹„", "ì—ë²„ì˜¨"]
    
    bep_jobs = get_bep_jobs()
    saramin_jobs = get_saramin_jobs(targets)
    
    scraped_data = bep_jobs + saramin_jobs
    
    if not scraped_data:
        print("\nâš ï¸  ìˆ˜ì§‘ëœ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    df_current = pd.DataFrame(scraped_data, columns=['company', 'title', 'experience', 'link'])
    df_current['link'] = df_current['link'].str.strip()
    df_current = df_current.drop_duplicates(subset=['link'])
    
    print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: ì´ {len(df_current)}ê±´")

    # 3. ì‹ ê·œ ê³µê³  ì²˜ë¦¬
    new_entries = df_current[~df_current['link'].isin(df_master['link'])].copy()
    
    if not new_entries.empty:
        print(f"\nğŸ†• ì‹ ê·œ ê³µê³  {len(new_entries)}ê±´ ë°œê²¬!")
        new_entries['first_seen'] = today
        
        if SLACK_WEBHOOK_URL:
            msg = f"ğŸ“¢ *ì‹ ê·œ ì±„ìš© ({len(new_entries)}ê±´)*\n"
            for _, r in new_entries.iterrows():
                msg += f"â€¢ [{r['company']}] {r['title']}\n  <{r['link']}|ë³´ê¸°>\n"
            requests.post(SLACK_WEBHOOK_URL, json={"text": msg})
        
        df_master = pd.concat([df_master, new_entries], ignore_index=True)
    else:
        print("\nâœ… ì‹ ê·œ ê³µê³  ì—†ìŒ")

    # 4. ì¢…ë£Œ ê³µê³  ì²˜ë¦¬
    active_links = df_current['link'].tolist()
    successfully_scraped_companies = df_current['company'].unique()
    
    is_missing = ~df_master['link'].isin(active_links)
    is_target_company = df_master['company'].isin(successfully_scraped_companies)
    
    closed_jobs = df_master[is_missing & is_target_company].copy()
    
    if not closed_jobs.empty:
        print(f"\nğŸ”š ì¢…ë£Œëœ ê³µê³  {len(closed_jobs)}ê±´ ì²˜ë¦¬")
        closed_jobs['completed_date'] = today
        df_comp = pd.concat([df_comp, closed_jobs], ignore_index=True)
        df_master = df_master[~(is_missing & is_target_company)]

    # 5. Encyclopedia ì—…ë°ì´íŠ¸
    print(f"\nğŸ“š ë°±ê³¼ì‚¬ì „ ì—…ë°ì´íŠ¸ ì¤‘...")
    
    # 5-1. ì‹ ê·œ ë§í¬ ì¶”ê°€
    new_for_ency = df_master[~df_master['link'].isin(df_ency['link'])].copy()
    
    if not new_for_ency.empty:
        print(f"  â€¢ ì‹ ê·œ ì—”íŠ¸ë¦¬ {len(new_for_ency)}ê±´ ì¶”ê°€")
        for _, row in new_for_ency.iterrows():
            new_row = pd.DataFrame([{
                'link': row['link'],
                'company': row['company'],
                'title': row['title'],
                'content': '',
                'first_seen': row['first_seen'],
                'completed_date': None,
                'last_updated': None
            }])
            df_ency = pd.concat([df_ency, new_row], ignore_index=True)
    
    # 5-2. ì¢…ë£Œ ê³µê³  ë‚ ì§œ ê¸°ë¡
    closed_links = closed_jobs['link'].tolist() if not closed_jobs.empty else []
    if closed_links:
        print(f"  â€¢ ì¢…ë£Œ ê³µê³  {len(closed_links)}ê±´ ë‚ ì§œ ê¸°ë¡")
        df_ency.loc[df_ency['link'].isin(closed_links), 'completed_date'] = today
    
    # 5-3. Content ìˆ˜ì§‘ (ë¬´íš¨í•œ content ì¬ìˆ˜ì§‘)
    needs_update = (
        (df_ency['content'].isna()) | 
        (df_ency['content'] == '') |
        (df_ency['content'].str.len() < 100) |
        (df_ency['content'].str.contains('ë¡œê·¸ì¸ì´ í•„ìš”í•œ|ìˆ˜ì§‘ ì‹¤íŒ¨|ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨', na=False)) |
        (df_ency['content'].apply(lambda x: is_invalid_content(str(x))))
    )
    
    # í™œì„± ê³µê³ ë§Œ ìˆ˜ì§‘
    active_and_needs = df_ency[needs_update & df_ency['link'].isin(active_links)]
    
    if not active_and_needs.empty:
        print(f"  â€¢ ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘ ëŒ€ìƒ: {len(active_and_needs)}ê±´")
        
        for idx, row in active_and_needs.iterrows():
            link = row['link']
            print(f"    [{list(active_and_needs.index).index(idx)+1}/{len(active_and_needs)}] {row['company']} - {row['title'][:30]}")
            
            content = fetch_detail_content(link)
            
            # ì¬ê²€ì¦
            if is_invalid_content(content):
                content = "ìƒì„¸ ë§í¬ ì°¸ì¡°"
            
            df_ency.loc[df_ency['link'] == link, ['content', 'last_updated']] = [content, today]
            time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

    # 6. ìµœì¢… ì €ì¥
    print(f"\nğŸ’¾ íŒŒì¼ ì €ì¥ ì¤‘...")
    
    for df in [df_master, df_comp, df_ency]:
        if 'link' in df.columns:
            df['link'] = df['link'].astype(str).str.strip()
            df.drop_duplicates(subset=['link'], keep='first', inplace=True)

    # ì •ë ¬
    if 'company' in df_ency.columns and 'first_seen' in df_ency.columns:
        df_ency = df_ency.sort_values(by=['company', 'first_seen'], ascending=[False, False])

    df_master.to_csv("job_listings_all.csv", index=False, encoding='utf-8-sig')
    df_comp.to_csv("Recruitment_completed.csv", index=False, encoding='utf-8-sig')
    df_ency.to_csv("encyclopedia.csv", index=False, encoding='utf-8-sig')
    
    print(f"\n{'='*60}")
    print(f"âœ… ì‘ì—… ì™„ë£Œ!")
    print(f"  â€¢ í˜„ì¬ ê³µê³ : {len(df_master)}ê±´")
    print(f"  â€¢ ì¢…ë£Œ ê³µê³ : {len(df_comp)}ê±´")
    print(f"  â€¢ ë°±ê³¼ì‚¬ì „: {len(df_ency)}ê±´")
    print(f"{'='*60}\n")

if __name__ == "__main__":
    main()
