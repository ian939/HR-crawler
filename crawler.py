import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
import re
import os

SLACK_WEBHOOK_URL = os.environ.get('SLACK_WEBHOOK_URL')
HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

def fetch_detail_content(url):
    """ìƒì„¸ í˜ì´ì§€ì˜ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ"""
    try:
        time.sleep(1) # ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ê°„ê²©
        res = requests.get(url, headers=HEADERS, timeout=10)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')

        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ë“±)
        for tag in soup(["script", "style", "nav", "footer", "header"]):
            tag.decompose()

        # ì‚¬ì´íŠ¸ë³„ ì£¼ìš” ë³¸ë¬¸ ì˜ì—­ ì¶”ì¶œ (íœ´ë¦¬ìŠ¤í‹± ë°©ì‹)
        if "saramin.co.kr" in url:
            # ì‚¬ëŒì¸ì€ ë³´í†µ .user_contentë‚˜ .job_detailì— ë‚´ìš©ì´ ìˆìŒ
            content = soup.select_one('.user_content') or soup.select_one('.job_detail')
        else:
            # BEP ë“± ê¸°íƒ€ ì‚¬ì´íŠ¸ìš©
            content = soup.select_one('main') or soup.select_one('#content') or soup.body

        if content:
            # ì¤„ë°”ê¿ˆê³¼ ê³µë°± ì •ë¦¬
            text = content.get_text(separator="\n", strip=True)
            return text
        return "ë³¸ë¬¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"

# --- ê¸°ì¡´ ìˆ˜ì§‘ í•¨ìˆ˜ (get_bep_jobs, get_saramin_jobs)ëŠ” ë™ì¼í•˜ê²Œ ìœ ì§€ ---
# (ê³µê°„ ì ˆì•½ì„ ìœ„í•´ í•¨ìˆ˜ ë‚´ë¶€ ë¡œì§ì€ ìƒëµí•˜ë©°, ì´ì „ ì½”ë“œì™€ ë™ì¼í•˜ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.)

def get_bep_jobs():
    # ... (ì´ì „ ì½”ë“œì™€ ë™ì¼)
    url = "https://bep.co.kr/Career/recruitment?type=3"
    jobs = []
    try:
        response = requests.get(url, headers=HEADERS)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        all_links = soup.find_all('a', href=re.compile(r'recruitmentView\?idx='))
        for link_tag in all_links:
            text = link_tag.get_text(" ", strip=True)
            if "ëª¨ì§‘ì¤‘" not in text: continue
            href = link_tag.get('href', '')
            full_link = f"https://bep.co.kr{href}" if not href.startswith('http') else href
            title = text.replace("ëª¨ì§‘ì¤‘", "").replace("ì „ê¸°ì°¨ì¶©ì „ì‚¬ì—…ë¶€ë¬¸", "").strip()
            jobs.append(['BEP', title, "ìƒì„¸ ì°¸ì¡°", full_link])
    except: pass
    return jobs

def get_saramin_jobs(companies):
    # ... (ì´ì „ ì½”ë“œì™€ ë™ì¼)
    base_url = "https://www.saramin.co.kr/zf_user/search/recruit"
    jobs = []
    for company in companies:
        try:
            params = {'searchword': company, 'searchType': 'search'}
            res = requests.get(base_url, headers=HEADERS, params=params)
            soup = BeautifulSoup(res.text, 'html.parser')
            items = soup.select('.item_recruit')
            for item in items:
                co_tag = item.select_one('.corp_name a')
                if not co_tag: continue
                co_name = co_tag.text.strip()
                if company in co_name.replace("(ì£¼)", "").replace("ì£¼ì‹íšŒì‚¬", ""):
                    title_tag = item.select_one('.job_tit a')
                    jobs.append([co_name, title_tag.text.strip(), "ìƒì„¸ ì°¸ì¡°", "https://www.saramin.co.kr" + title_tag['href']])
            time.sleep(1)
        except: pass
    return jobs

def send_slack_message(new_jobs):
    if not SLACK_WEBHOOK_URL or not new_jobs: return
    message = f"ğŸ“¢ *ì‹ ê·œ ì±„ìš© ê³µê³  ({len(new_jobs)}ê±´)*\n\n"
    for job in new_jobs:
        message += f"â€¢ *[{job[0]}]* {job[1]}\n  <{job[3]}|ê³µê³  ë³´ê¸°>\n\n"
    requests.post(SLACK_WEBHOOK_URL, json={"text": message})

def main():
    saramin_target = ["ëŒ€ì˜ì±„ë¹„", "ì´ë¸Œì´ì‹œìŠ¤", "í”ŒëŸ¬ê·¸ë§í¬", "ë³¼íŠ¸ì—…", "ì°¨ì§€ë¹„", "ì—ë²„ì˜¨"]
    today_str = datetime.now().strftime('%Y-%m-%d')
    
    # 1. ëª©ë¡ í¬ë¡¤ë§
    scraped_data = get_bep_jobs() + get_saramin_jobs(saramin_target)
    df_current = pd.DataFrame(scraped_data, columns=['company', 'title', 'experience', 'link'])
    
    master_file = "job_listings_all.csv"
    completed_file = "Recruitment_completed.csv"
    encyclopedia_file = "encyclopedia.csv"

    # 2. ë°±ê³¼ì‚¬ì „(Encyclopedia) ë¡œë“œ ë° ì‹ ê·œ ë‚´ìš© ìˆ˜ì§‘
    if os.path.exists(encyclopedia_file):
        df_encyclopedia = pd.read_csv(encyclopedia_file)
    else:
        df_encyclopedia = pd.DataFrame(columns=['link', 'company', 'title', 'content', 'last_updated'])

    # ì•„ì§ ë°±ê³¼ì‚¬ì „ì— ì—†ëŠ” ë§í¬ë“¤ë§Œ í•„í„°ë§
    new_links_to_fetch = df_current[~df_current['link'].isin(df_encyclopedia['link'])]

    if not new_links_to_fetch.empty:
        print(f"{len(new_links_to_fetch)}ê°œì˜ ìƒˆë¡œìš´ ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘ ì‹œì‘...")
        new_details = []
        for _, row in new_links_to_fetch.iterrows():
            content = fetch_detail_content(row['link'])
            new_details.append({
                'link': row['link'],
                'company': row['company'],
                'title': row['title'],
                'content': content,
                'last_updated': today_str
            })
        
        # ìƒˆë¡œìš´ ìƒì„¸ ë‚´ìš©ì„ ë°±ê³¼ì‚¬ì „ì— ì¶”ê°€
        df_new_ency = pd.DataFrame(new_details)
        df_encyclopedia = pd.concat([df_encyclopedia, df_new_ency], ignore_index=True)
        df_encyclopedia.to_csv(encyclopedia_file, index=False, encoding='utf-8-sig')

    # 3. ê¸°ì¡´ ë§ˆìŠ¤í„°/ì™„ë£Œ ë¡œì§ (ë™ì¼í•˜ê²Œ ì‘ë™)
    if os.path.exists(master_file):
        df_master = pd.read_csv(master_file)
    else:
        df_master = pd.DataFrame(columns=['company', 'title', 'experience', 'link', 'first_seen'])

    df_new_jobs = df_current[~df_current['link'].isin(df_master['link'])].copy()
    if not df_new_jobs.empty:
        df_new_jobs['first_seen'] = today_str
        send_slack_message(df_new_jobs.values.tolist())
    
    df_closed = df_master[~df_master['link'].isin(df_current['link'])].copy()
    if not df_closed.empty:
        df_closed['completed_date'] = today_str
        if os.path.exists(completed_file):
            df_comp_history = pd.read_csv(completed_file)
            df_comp_history = pd.concat([df_comp_history, df_closed], ignore_index=True)
        else:
            df_comp_history = df_closed
        df_comp_history.to_csv(completed_file, index=False, encoding='utf-8-sig')

    df_still_active = df_master[df_master['link'].isin(df_current['link'])]
    df_final_master = pd.concat([df_still_active, df_new_jobs], ignore_index=True)
    df_final_master.to_csv(master_file, index=False, encoding='utf-8-sig')
    
    print("ëª¨ë“  ì‘ì—… ì™„ë£Œ!")

if __name__ == "__main__":
    main()
