import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
import re
import os

# --- í™˜ê²½ ì„¤ì • ---
SLACK_WEBHOOK_URL = os.environ.get('SLACK_WEBHOOK_URL')
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
}

def fetch_detail_content(url):
    """ìƒì„¸ ë³¸ë¬¸ ì¶”ì¶œ: í…ìŠ¤íŠ¸ ìš°ì„ , ë¶€ì¡±í•˜ë©´ ì´ë¯¸ì§€ URL ìˆ˜ì§‘"""
    try:
        time.sleep(1.5)
        target_url = url
        if "saramin.co.kr" in url and "rec_idx=" in url:
            rec_idx_match = re.search(r'rec_idx=(\d+)', url)
            if rec_idx_match:
                target_url = f"https://www.saramin.co.kr/zf_user/jobs/relay/view-detail?rec_idx={rec_idx_match.group(1)}"

        res = requests.get(target_url, headers=HEADERS, timeout=15)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')

        for tag in soup(["script", "style", "nav", "footer", "header", "button", "aside"]):
            tag.decompose()

        selectors = ['.user_content', '.recruit_view_cont', '.view_con', '.job_detail', '.template_area', '.section_view']
        content_area = None
        for sel in selectors:
            content_area = soup.select_one(sel)
            if content_area: break
        
        if not content_area: content_area = soup.body

        text_content = content_area.get_text(separator="\n", strip=True) if content_area else ""
        
        poor_keywords = ["ì±„ìš©ê³µê³  ìƒì„¸", "ë³¸ë¬¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ë¡œê·¸ì¸"]
        is_poor = len(text_content) < 150 or any(text_content.strip() == k for k in poor_keywords)

        if is_poor and content_area:
            imgs = content_area.find_all('img')
            img_urls = [ (img.get('src') or img.get('data-src')) for img in imgs ]
            clean_imgs = [ "https:" + i if i.startswith('//') else i for i in img_urls if i and not any(x in i.lower() for x in ["icon", "logo", "common"])]
            if clean_imgs:
                return "[ì´ë¯¸ì§€ ê³µê³ ] " + ", ".join(clean_imgs)

        return text_content[:20000] if len(text_content) > 50 else "ìƒì„¸ ë‚´ìš©ì€ ë§í¬ë¥¼ ì°¸ì¡°í•´ ì£¼ì„¸ìš”."
    except Exception as e:
        return f"ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"

def get_bep_jobs():
    """BEP(ì›Œí„°) ì „ê¸°ì°¨ì¶©ì „ì‚¬ì—…ë¶€ë¬¸ ìˆ˜ì§‘ ë³´ì™„"""
    url = "https://bep.co.kr/Career/recruitment?type=3"
    jobs = []
    try:
        # ì„¸ì…˜ì„ ì‚¬ìš©í•˜ì—¬ ì¿ í‚¤ ë“± ìœ ì§€
        with requests.Session() as s:
            response = s.get(url, headers=HEADERS, timeout=15)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # BEP ì‚¬ì´íŠ¸ì˜ ê³µê³  ë¦¬ìŠ¤íŠ¸ëŠ” ë³´í†µ 'board_list' ë˜ëŠ” 'table' êµ¬ì¡° ë‚´ì˜ a íƒœê·¸ì— ì¡´ì¬
            # ìƒì„¸ í˜ì´ì§€ ë§í¬ íŒ¨í„´: /Career/recruitmentView?idx=...
            links = soup.find_all('a', href=re.compile(r'recruitmentView\?idx='))
            
            for l in links:
                # ì œëª© ì¶”ì¶œ (ë‚´ë¶€ spanì´ë‚˜ strong íƒœê·¸ê°€ ìˆì„ ìˆ˜ ìˆìŒ)
                title_text = l.get_text(" ", strip=True)
                if not title_text or "ëª©ë¡" in title_text or "ì´ì „ê¸€" in title_text or "ë‹¤ìŒê¸€" in title_text:
                    continue
                
                href = l.get('href', '')
                full_link = f"https://bep.co.kr{href}" if href.startswith('/') else href
                if "idx=" not in full_link: continue
                
                clean_title = title_text.replace("ëª¨ì§‘ì¤‘", "").strip()
                jobs.append(['BEP(ì›Œí„°)', clean_title, "ê³µê³  í™•ì¸", full_link])
                
            # ì¤‘ë³µ ì œê±° (ìˆ˜ì§‘ ë‹¨ê³„)
            unique_jobs = []
            seen_links = set()
            for j in jobs:
                if j[3] not in seen_links:
                    unique_jobs.append(j)
                    seen_links.add(j[3])
            return unique_jobs
    except Exception as e:
        print(f"BEP ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

def get_saramin_jobs(companies):
    """ì‚¬ëŒì¸ ìˆ˜ì§‘"""
    base_url = "https://www.saramin.co.kr/zf_user/search/recruit"
    jobs = []
    for company in companies:
        try:
            params = {'searchword': company, 'searchType': 'search'}
            res = requests.get(base_url, headers=HEADERS, params=params, timeout=15)
            soup = BeautifulSoup(res.text, 'html.parser')
            items = soup.select('.item_recruit')
            for item in items:
                co_tag = item.select_one('.corp_name a')
                if not co_tag: continue
                co_name = co_tag.text.strip()
                if company in co_name.replace("(ì£¼)", "").replace("ì£¼ì‹íšŒì‚¬", ""):
                    title_tag = item.select_one('.job_tit a')
                    conds = item.select('.job_condition span')
                    exp = conds[1].text.strip() if len(conds) > 1 else "ìƒì„¸ ì°¸ì¡°"
                    jobs.append([co_name, title_tag.text.strip(), exp, "https://www.saramin.co.kr" + title_tag['href']])
            time.sleep(1.2)
        except: continue
    return jobs

def safe_load_df(file_path, default_cols):
    """íŒŒì¼ ë¡œë“œ ë° í‘œì¤€í™” (ì¤‘ë³µ ì œê±° í¬í•¨)"""
    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            df.columns = [c.strip().replace('\ufeff', '') for c in df.columns]
            for col in default_cols:
                if col not in df.columns: df[col] = ""
            # ë¡œë“œ ì‹œì ì— ì´ë¯¸ ìˆëŠ” ì¤‘ë³µ ì œê±°
            if 'link' in df.columns:
                df = df.drop_duplicates(subset=['link'], keep='first')
            return df[default_cols]
        except:
            return pd.DataFrame(columns=default_cols)
    return pd.DataFrame(columns=default_cols)

def main():
    saramin_targets = ["ëŒ€ì˜ì±„ë¹„", "ì´ë¸Œì´ì‹œìŠ¤", "í”ŒëŸ¬ê·¸ë§í¬", "ë³¼íŠ¸ì—…", "ì°¨ì§€ë¹„", "ì—ë²„ì˜¨"]
    today = datetime.now().strftime('%Y-%m-%d')
    
    # 1. ë°ì´í„° ë¡œë“œ
    df_master = safe_load_df("job_listings_all.csv", ['company', 'title', 'experience', 'link', 'first_seen'])
    df_ency = safe_load_df("encyclopedia.csv", ['link', 'company', 'title', 'content', 'last_updated'])
    df_comp = safe_load_df("Recruitment_completed.csv", ['company', 'title', 'experience', 'link', 'completed_date'])

    print(f"[{today}] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    scraped = get_bep_jobs() + get_saramin_jobs(saramin_targets)
    df_current = pd.DataFrame(scraped, columns=['company', 'title', 'experience', 'link'])

    if df_current.empty:
        print("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # ì¤‘ë³µ ì œê±° (í˜„ì¬ ìˆ˜ì§‘ë¶„ ë‚´)
        df_current = df_current.drop_duplicates(subset=['link'])

        # 2. ì‹ ê·œ ê³µê³  ë° ìŠ¬ë™ ì•Œë¦¼
        # ì´ë¯¸ masterì— ìˆëŠ” ë§í¬ëŠ” ì œì™¸
        new_entries = df_current[~df_current['link'].isin(df_master['link'])].copy()
        
        if not new_entries.empty:
            new_entries['first_seen'] = today
            if SLACK_WEBHOOK_URL:
                msg = f"ğŸ“¢ *ì‹ ê·œ ì±„ìš© ({len(new_entries)}ê±´)*\n"
                for _, r in new_entries.iterrows(): 
                    msg += f"â€¢ [{r['company']}] {r['title']} ({r['experience']})\n  <{r['link']}|ë³´ê¸°>\n"
                requests.post(SLACK_WEBHOOK_URL, json={"text": msg})
            
            # ì‹ ê·œ ë°ì´í„°ë§Œ masterì— ì¶”ê°€
            df_master = pd.concat([df_master, new_entries], ignore_index=True).drop_duplicates(subset=['link'])

        # 3. ì±„ìš© ì¢…ë£Œ ì²˜ë¦¬ (ìˆ˜ì§‘ ëŒ€ìƒ ê¸°ì—… ì¤‘ í˜„ì¬ ê³µê³ ì— ì—†ëŠ” ê²ƒ)
        active_cos = df_current['company'].unique()
        is_missing = ~df_master['link'].isin(df_current['link'])
        is_target_co = df_master['company'].isin(active_cos)
        
        closed = df_master[is_missing & is_target_co].copy()
        if not closed.empty:
            closed['completed_date'] = today
            # ì™„ë£Œ íŒŒì¼ì— ì¶”ê°€ í›„ ì¤‘ë³µ ì œê±°
            df_comp = pd.concat([df_comp, closed], ignore_index=True).drop_duplicates(subset=['link'])
            # Masterì—ì„œ ì‚­ì œ
            df_master = df_master[~(is_missing & is_target_co)]

    # 4. Encyclopedia ì—…ë°ì´íŠ¸ (ë³¸ë¬¸/ì´ë¯¸ì§€ ìˆ˜ì§‘)
    # ë‚´ìš©ì´ ë¶€ì‹¤í•˜ê±°ë‚˜ ì—†ëŠ” í•­ëª© ì¶”ì¶œ
    retry_keywords = ["ì±„ìš©ê³µê³  ìƒì„¸", "ë³¸ë¬¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ë¡œê·¸ì¸", "ìƒì„¸ ì°¸ì¡°", "ìˆ˜ì§‘ ì‹¤íŒ¨"]
    
    # ê¸°ì¡´ ë°ì´í„° ì¤‘ ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œ ê²ƒ
    df_ency['content'] = df_ency['content'].fillna("")
    is_bad = df_ency['content'].apply(lambda x: any(k in str(x) for k in retry_keywords) or len(str(x)) < 150)
    retry_links = df_ency[is_bad]['link'].tolist()
    
    # ì•„ì˜ˆ ë°±ê³¼ì‚¬ì „ì— ì—†ëŠ” ì‹ ê·œ ë§í¬
    new_links = df_master[~df_master['link'].isin(df_ency['link'])]['link'].tolist()
    
    target_links = list(set(retry_links + new_links))

    if target_links:
        print(f"ìƒì„¸ ë‚´ìš©/ì´ë¯¸ì§€ {len(target_links)}ê±´ ì²˜ë¦¬ ì¤‘...")
        for link in target_links:
            # ì›ë³¸ ì •ë³´ ì°¾ê¸°
            source = df_master[df_master['link'] == link]
            if source.empty: continue
            
            row = source.iloc[0]
            content = fetch_detail_content(link)
            
            if link in df_ency['link'].values:
                df_ency.loc[df_ency['link'] == link, ['content', 'last_updated']] = [content, today]
            else:
                new_row = pd.DataFrame([{
                    'link': link, 'company': row['company'], 'title': row['title'], 
                    'content': content, 'last_updated': today
                }])
                df_ency = pd.concat([df_ency, new_row], ignore_index=True)
        
        # ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ë²ˆ ë” ì¤‘ë³µ ì œê±°
        df_ency = df_ency.drop_duplicates(subset=['link'], keep='last')

    # 5. íŒŒì¼ ì €ì¥
    df_master.to_csv("job_listings_all.csv", index=False, encoding='utf-8-sig')
    df_comp.to_csv("Recruitment_completed.csv", index=False, encoding='utf-8-sig')
    df_ency.to_csv("encyclopedia.csv", index=False, encoding='utf-8-sig')
    print(f"ì‘ì—… ì™„ë£Œ. (í˜„ì¬ í™œì„± ê³µê³ : {len(df_master)}ê±´)")

if __name__ == "__main__":
    main()
